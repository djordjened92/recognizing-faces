{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train'\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "img_paths = glob.glob(os.path.join(f'{TRAIN_PATH}/*/*/*.jpg'))\n",
    "labels = [os.path.split(p)[0] for p in img_paths]\n",
    "persons = list(set(labels))\n",
    "persons_enc = dict(zip(persons, range(len(persons))))\n",
    "labels = list(map(lambda x: persons_enc[x], labels))\n",
    "num_classes = len(persons)\n",
    "\n",
    "# Create validation set manually by sampling one image for each person\n",
    "train_x, train_y, val_x, val_y = [], [], [], []\n",
    "idx_examples = defaultdict(list)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    idx_examples[labels[i]].append(img_paths[i])\n",
    "\n",
    "for k, v in idx_examples.items():\n",
    "    val_y.append(k)\n",
    "    train_y.extend([k]*(len(v) - 1))\n",
    "    val_x.append(v[0])\n",
    "    train_x.extend(v[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(img_paths, labels, preprocess, batch_size, input_shape, num_classes, shuffle=True):\n",
    "    indexes = np.arange(len(img_paths))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        counter = 0\n",
    "        for idx in indexes:\n",
    "            img = image.load_img(img_paths[idx],\n",
    "                                 target_size=(input_shape[0], input_shape[1]))\n",
    "            label = labels[idx]\n",
    "            \n",
    "            X.append(np.array(img).astype(np.float32))\n",
    "            y.append(label)\n",
    "            \n",
    "            if len(y) == batch_size:\n",
    "                yield (preprocess(np.array(X)), keras.utils.to_categorical(np.array(y), num_classes=num_classes))\n",
    "                X, y = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet(input_shape, l2_value, dropout, model_name='mobilenet'):\n",
    "    mobile = MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        dropout=dropout,\n",
    "        include_top=False,\n",
    "        pooling='max',\n",
    "        alpha=1.,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    for layer in mobile.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = layers.Dense(1024, kernel_regularizer=l2(l2_value), activation='relu')(mobile.output)\n",
    "    x = layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    return Model(mobile.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "l2_value = 1e-9\n",
    "dropout = 0.1\n",
    "batch_size = 48\n",
    "num_epochs = 1000\n",
    "temperature = 0.1\n",
    "width = 1024\n",
    "contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 10:\n",
    "        return 0.5 * lr\n",
    "    elif epoch == 180:\n",
    "        return 0.8 * lr\n",
    "    elif epoch == 250:\n",
    "        return 0.5 * lr\n",
    "    elif epoch == 300:\n",
    "        return 0.7 * lr\n",
    "    elif epoch == 400:\n",
    "        return 0.3 *lr\n",
    "    elif epoch == 700:\n",
    "        return 0.6 *lr\n",
    "    return lr\n",
    "    \n",
    "lr_callback = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f908e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = batching(train_x, train_y, mobilenet_preprocess, batch_size, input_shape, num_classes)\n",
    "val_gen = batching(val_x, val_y, mobilenet_preprocess, batch_size, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = mobilenet(input_shape, l2_value, dropout)\n",
    "print(f'NN number of parameters: {base_network.count_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distorts the color distibutions of images\n",
    "class RandomColorAffine(layers.Layer):\n",
    "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.brightness = brightness\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if training:\n",
    "            batch_size = tf.shape(images)[0]\n",
    "\n",
    "            # Same for all colors\n",
    "            brightness_scales = 1 + tf.random.uniform(\n",
    "                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n",
    "            )\n",
    "            # Different for all colors\n",
    "            jitter_matrices = tf.random.uniform(\n",
    "                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n",
    "            )\n",
    "\n",
    "            color_transforms = (\n",
    "                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n",
    "                + jitter_matrices\n",
    "            )\n",
    "            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n",
    "        return images\n",
    "\n",
    "# Image augmentation module\n",
    "def get_augmenter(min_area, brightness, jitter):\n",
    "    zoom_factor = 1.0 - tf.sqrt(min_area)\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            layers.experimental.preprocessing.Rescaling(1 / 255),\n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n",
    "            layers.experimental.preprocessing.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n",
    "            RandomColorAffine(brightness, jitter),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation from Keras code examples\n",
    "# Define the contrastive model with model-subclassing\n",
    "class ContrastiveModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n",
    "        self.encoder = base_network\n",
    "        \n",
    "        # Non-linear MLP as projection head\n",
    "        self.projection_head = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(width,)),\n",
    "                layers.Dense(width, activation=\"relu\"),\n",
    "                layers.Dense(width),\n",
    "            ],\n",
    "            name=\"projection_head\",\n",
    "        )\n",
    "        self.encoder.summary()\n",
    "        self.projection_head.summary()\n",
    "\n",
    "    def compile(self, contrastive_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.contrastive_optimizer = contrastive_optimizer\n",
    "\n",
    "        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"c_acc\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.contrastive_loss_tracker,\n",
    "            self.contrastive_accuracy\n",
    "        ]\n",
    "\n",
    "    def contrastive_loss(self, projections_1, projections_2):\n",
    "        # InfoNCE loss (information noise-contrastive estimation)\n",
    "        # NT-Xent loss (normalized temperature-scaled cross entropy)\n",
    "\n",
    "        # Cosine similarity: the dot product of the l2-normalized feature vectors\n",
    "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
    "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
    "        similarities = (\n",
    "            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n",
    "        )\n",
    "\n",
    "        # The similarity between the representations of two augmented views of the\n",
    "        # same image should be higher than their similarity with other views\n",
    "        batch_size = tf.shape(projections_1)[0]\n",
    "        contrastive_labels = tf.range(batch_size)\n",
    "        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n",
    "        self.contrastive_accuracy.update_state(\n",
    "            contrastive_labels, tf.transpose(similarities)\n",
    "        )\n",
    "\n",
    "        # The temperature-scaled similarities are used as logits for cross-entropy\n",
    "        # a symmetrized version of the loss is used here\n",
    "        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, similarities, from_logits=True\n",
    "        )\n",
    "        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, tf.transpose(similarities), from_logits=True\n",
    "        )\n",
    "        return (loss_1_2 + loss_2_1) / 2\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, _ = data\n",
    "        \n",
    "        # Each image is augmented twice, differently\n",
    "        augmented_images_1 = self.contrastive_augmenter(images, training=True)\n",
    "        augmented_images_2 = self.contrastive_augmenter(images, training=True)\n",
    "        with tf.GradientTape() as tape:\n",
    "            features_1 = self.encoder(augmented_images_1, training=True)\n",
    "            features_2 = self.encoder(augmented_images_2, training=True)\n",
    "            # The representations are passed through a projection mlp\n",
    "            projections_1 = self.projection_head(features_1, training=True)\n",
    "            projections_2 = self.projection_head(features_2, training=True)\n",
    "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
    "        gradients = tape.gradient(\n",
    "            contrastive_loss,\n",
    "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "        )\n",
    "        self.contrastive_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients,\n",
    "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "            )\n",
    "        )\n",
    "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        images, _ = data\n",
    "        \n",
    "        # Each image is augmented twice, differently\n",
    "        augmented_images_1 = self.contrastive_augmenter(images, training=False)\n",
    "        augmented_images_2 = self.contrastive_augmenter(images, training=False)\n",
    "        \n",
    "        features_1 = self.encoder(augmented_images_1, training=False)\n",
    "        features_2 = self.encoder(augmented_images_2, training=False)\n",
    "        # The representations are passed through a projection mlp\n",
    "        projections_1 = self.projection_head(features_1, training=False)\n",
    "        projections_2 = self.projection_head(features_2, training=False)\n",
    "        contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
    "        \n",
    "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, images):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de42077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet_contrastive'\n",
    "ckpt_dir = os.path.join('pretrained/checkpoints', model_name)\n",
    "\n",
    "# Contrastive pretraining\n",
    "pretraining_model = ContrastiveModel()\n",
    "pretraining_model.compile(\n",
    "    contrastive_optimizer=keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "class SaveCheckpointCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, ckpt_dir):\n",
    "        super(SaveCheckpointCallback, self).__init__()\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        self.val_loss = 1e10\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        curr_loss = logs['val_c_loss']\n",
    "        \n",
    "        if curr_loss < self.val_loss:\n",
    "            self.val_loss = curr_loss\n",
    "            base_network.save(os.path.join(ckpt_dir, 'model.hdf5'), overwrite=True)\n",
    "\n",
    "pretraining_model.fit(train_gen,\n",
    "                      validation_data=val_gen,\n",
    "                      epochs=num_epochs,\n",
    "                      steps_per_epoch=len(train_x)//batch_size,\n",
    "                      validation_steps = len(val_x)//batch_size,\n",
    "                      callbacks=[SaveCheckpointCallback(ckpt_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77910b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
