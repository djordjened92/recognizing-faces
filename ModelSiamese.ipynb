{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.applications import MobileNet, ResNet50\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv1D, Attention, GlobalAveragePooling1D, BatchNormalization\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "random.seed(123)\n",
    "tf.random.set_seed(12)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input embeddings\n",
    "\n",
    "The data in the input pickle file is stored in a dictionary structure:\n",
    "```\n",
    "{\n",
    "    [\n",
    "        'FAMILY_ID/PERSON_ID': [EMB_1, EMB_2...EMB_N],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_img_embeddings.pkl', 'rb') as f:\n",
    "       train_embeddings = pickle.load(f)\n",
    "print(f'The keys examples: {list(train_embeddings.keys())[:5]}')\n",
    "\n",
    "embedding_shape = list(train_embeddings.values())[0].shape\n",
    "print(f'Embeddings shape: {embedding_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Available training pairs from csv files are splitted to train - validation sets. Those pairs are positive(there is blood relation). For each set(train/valid) we additionally generate negative pairs.\n",
    "\n",
    "Positive pairs are generated according to the input csv file. For each person of positive pair we create one negative pair.\n",
    "In total we'll have twice more negative than positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_pair(pair):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    '''\n",
    "    def emb_to_3channel(emb):\n",
    "#         emb_1ch = np.concatenate([emb, emb], axis=0)\n",
    "#         emb_2ch = np.concatenate([emb, emb[::-1,:]], axis=0)\n",
    "#         emb_3ch = np.concatenate([emb[::-1,:], emb], axis=0)\n",
    "#         emb_mc = np.concatenate([emb_1ch, emb_2ch, emb_3ch], axis=-1)\n",
    "        emb_pad = np.concatenate([emb, np.zeros(emb.shape)], axis=0)\n",
    "        emb_mc = np.tile(emb_pad, (1, 3))\n",
    "        emb_mc = np.reshape(emb_mc, (32, 32, 3))\n",
    "        return emb_mc\n",
    "        \n",
    "    p1, p2 = pair\n",
    "    img_path1 = p1.replace('/', '\\\\')\n",
    "    img_path2 = p2.replace('/', '\\\\')\n",
    "    \n",
    "    dir1 = np.expand_dims(train_embeddings[img_path1], axis=-1)\n",
    "    dir2 = np.expand_dims(train_embeddings[img_path2], axis=-1)\n",
    "    \n",
    "    for i in range(len(dir1)):\n",
    "        for j in range(len(dir2)):\n",
    "#             yield emb_to_3channel(dir1[i]), emb_to_3channel(dir2[j])\n",
    "            yield dir1[i], dir2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs):\n",
    "    for pair, label in input_pairs:\n",
    "        try:\n",
    "            emb_pairs = make_image_pair(pair)\n",
    "            for emb_pair in emb_pairs:\n",
    "                yield emb_pair, label\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "def batched_pairs(input_pairs, batch_size, dataset_period):\n",
    "    embs1 = []\n",
    "    embs2 = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    for example in pairs_set(input_pairs):\n",
    "        # Get every nth sample\n",
    "        counter += 1\n",
    "        if counter % dataset_period:\n",
    "            continue\n",
    "        \n",
    "        embs, label = example\n",
    "        emb1, emb2 = embs\n",
    "        embs1.append(emb1)\n",
    "        embs2.append(emb2)\n",
    "        labels.append(label)\n",
    "        if len(labels) == batch_size:\n",
    "            yield {'input_1:0':np.array(embs1), 'input_2:0':np.array(embs2)}, np.array(labels).astype(float)\n",
    "            embs1, embs2, labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_val_set.json', 'r') as f:\n",
    "    train_val_set = json.load(f)\n",
    "\n",
    "train_rlt_list, neg_train_rltshps, valid_rlt_list, neg_valid_rltshps = list(train_val_set.values())\n",
    "train_rlts = list(zip(train_rlt_list + neg_train_rltshps, [True]*len(train_rlt_list) + [False]*len(neg_train_rltshps)))\n",
    "val_rlts = list(zip(valid_rlt_list + neg_valid_rltshps, [True]*len(valid_rlt_list) + [False]*len(neg_valid_rltshps)))\n",
    "\n",
    "random.shuffle(train_rlts)\n",
    "random.shuffle(val_rlts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network\n",
    "\n",
    "Initial experimenting is done with conv1D deep neural network, as additional option for experimenting there is simple attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1D_model(input_shape, l2_value, dropout):\n",
    "    '''\n",
    "    Create deep Keras model.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input layer\n",
    "    \n",
    "    Returns:\n",
    "    Model -- Keras model\n",
    "    '''\n",
    "    def residual(x, kernel, l2_value, activation='relu'):\n",
    "        x1 = Conv1D(x.shape[-1], kernel, kernel_regularizer=l2(l2_value), activation=activation, padding='same')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        return x + x1\n",
    "    \n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv1D(1, 33, kernel_regularizer=l2(l2_value), activation='relu')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    '''\n",
    "    x = Conv1D(input.shape[1] // 128, 3, kernel_regularizer=l2(l2_value), activation='relu')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 3, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 64, 11, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 5, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 64, 5, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 5, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 64, 7, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 7, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 64, 7, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 7, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 32, 11, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 11, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 32, 17, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 17, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 32, 17, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 17, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 32, 17, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 17, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 8, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 8, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    \n",
    "    x = Conv1D(input.shape[1] // 8, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = residual(x, 19, l2_value, 'relu')\n",
    "    '''\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(input.shape[1] // 16, kernel_regularizer=l2(l2_value), activation='relu', name='inference')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "#     x = Dense(input.shape[1] // 16, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def mobilenet(input_shape, l2_value, dropout):\n",
    "    mobile = MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        dropout=dropout,\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        alpha=0.5,\n",
    "        weights=None\n",
    "    )\n",
    "    \n",
    "    for layer in mobile.layers:\n",
    "        layer.trainable = True\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = Dense(64, kernel_regularizer=l2(l2_value), activation='tanh')(mobile.output)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(32, kernel_regularizer=l2(l2_value), activation='tanh')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return Model(mobile.input, x)\n",
    "\n",
    "def resnet50(input_shape, l2_value):\n",
    "    mobile = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        weights=None\n",
    "    )\n",
    "    \n",
    "    for layer in mobile.layers:\n",
    "        layer.trainable = True\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = Dense(64, kernel_regularizer=l2(l2_value), activation='tanh')(mobile.output)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(32, kernel_regularizer=l2(l2_value), activation='tanh')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return Model(mobile.input, x)\n",
    "\n",
    "def attention_model(input_shape, train_mode=True):\n",
    "    '''\n",
    "    Inspired by code example:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "    '''\n",
    "    input = Input(shape=input_shape, dtype='int32')\n",
    "    query_input = value_input = K.squeeze(input, -1)\n",
    "    \n",
    "    # Embedding lookup.\n",
    "    token_embedding = tf.keras.layers.Embedding(input_dim=input_shape[1], output_dim=64)\n",
    "    # Query embeddings of shape [batch_size, Tq, dimension].\n",
    "    query_embeddings = token_embedding(query_input)\n",
    "    # Value embeddings of shape [batch_size, Tv, dimension].\n",
    "    value_embeddings = token_embedding(value_input)\n",
    "\n",
    "    query_seq_encoding = Conv1D(input.shape[1] // 4, 5, activation='relu', padding='same')(\n",
    "        query_embeddings)\n",
    "    value_seq_encoding = Conv1D(input.shape[1] // 4, 5, activation='relu', padding='same')(\n",
    "        value_embeddings)\n",
    "    \n",
    "    query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "        [query_seq_encoding, value_seq_encoding], training=train_mode)\n",
    "    \n",
    "    # Reduce over the sequence axis to produce encodings of shape\n",
    "    # [batch_size, filters].\n",
    "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_seq_encoding)\n",
    "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_value_attention_seq)\n",
    "    \n",
    "    # Concatenate query and document encodings to produce a DNN input layer.\n",
    "    attn_out_layer = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\n",
    "    return Model(input, attn_out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 3.0\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    sum_square = K.sum(K.square(x - y), axis=1)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    x, y = vectors\n",
    "    x_norm = tf.norm(x, axis=1)\n",
    "    y_norm = tf.norm(y, axis=1)\n",
    "    x_y_dot = tf.einsum('ij,ij->i', x, y)\n",
    "    cos_sim = x_y_dot / (x_norm * y_norm + K.epsilon())\n",
    "    return 1. - cos_sim\n",
    "\n",
    "def cos_euc_dist(vectors):\n",
    "    euc = euclidean_distance(vectors)\n",
    "    cos_sim = cosine_similarity(vectors)\n",
    "    return (1. - cos_sim) * euc\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    alpha = 1.\n",
    "    gamma = 1.5\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(MARGIN - y_pred, 0))\n",
    "    weight = 30. * y_true + 1.*(1 - y_true)#alpha * K.pow(1 - y_pred, gamma)\n",
    "    return K.mean(weight * (y_true * square_pred + (1 - y_true) * margin_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "l2_value = 1e-4\n",
    "dropout = 0.3\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "dataset_period = 1\n",
    "model_name = 'conv1D_model'\n",
    "# 'euclidian' or 'cosine'\n",
    "distance_type = 'euclidean_distance'\n",
    "optimizer = 'Adam'\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    elif epoch < 100:\n",
    "        return lr / 1.5\n",
    "    elif epoch < 2000:\n",
    "        return lr / 2\n",
    "    else:\n",
    "        return lr / 10\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Create dictionary of parameters for saving configuration\n",
    "train_config = {}\n",
    "for name in [\n",
    "    'learning_rate',\n",
    "    'l2_value',\n",
    "    'dropout',\n",
    "    'epochs',\n",
    "    'batch_size',\n",
    "    'model_name',\n",
    "    'dataset_period',\n",
    "    'distance_type',\n",
    "    'optimizer'\n",
    "]:\n",
    "    train_config[name] = eval(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the conv_1D_model input shape is (512, 1),\n",
    "# for the mobilenet it is (32, 32, 3)\n",
    "input_shape = (512, 1)\n",
    "base_network = eval(model_name)(input_shape, l2_value, dropout)\n",
    "base_network.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Siamese network\n",
    "input1 = Input(shape=input_shape)\n",
    "input2 = Input(shape=input_shape)\n",
    "processed1 = base_network(input1)\n",
    "processed2 = base_network(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_function = eval(distance_type)\n",
    "distance = Lambda(dist_function,\n",
    "                  output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "model = Model([input1, input2], distance)\n",
    "optimizer = eval(optimizer)(learning_rate=learning_rate)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard plugin in order to track changes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs --port=7007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation labels for the purpose of the metrics callback\n",
    "val_labels = []\n",
    "train_labels = []\n",
    "for e in batched_pairs(val_rlts, 1, dataset_period):\n",
    "    val_labels.append(e[1][0])\n",
    "\n",
    "for e in batched_pairs(train_rlts, 1, dataset_period):\n",
    "    train_labels.append(e[1][0])\n",
    "\n",
    "val_len = len(val_labels)\n",
    "train_len = len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_distance_stats(predictions, labels):\n",
    "    val_pos = predictions[labels.astype(np.bool)]\n",
    "    val_neg = predictions[(1 - labels).astype(np.bool)]\n",
    "    val_pos_m, val_pos_s = np.mean(val_pos), np.std(val_pos)\n",
    "    val_neg_m, val_neg_s = np.mean(val_neg), np.std(val_neg)\n",
    "    \n",
    "    return val_pos_m, val_pos_s, val_neg_m, val_neg_s\n",
    "    \n",
    "    \n",
    "class MetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir):\n",
    "        super(Callback, self).__init__()\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        self.train_writer = tf.summary.create_file_writer(logdir + '/train')\n",
    "        self.valid_writer = tf.summary.create_file_writer(logdir + '/valid')\n",
    "        self.step_number = 0\n",
    "        \n",
    "    def tb_writer(self, items_to_write, wtype):\n",
    "        writer = self.train_writer if wtype == 'train' else self.valid_writer\n",
    "        \n",
    "        with writer.as_default():\n",
    "            for name, value in items_to_write.items():\n",
    "                tf.summary.scalar(name, value, self.step_number)\n",
    "            writer.flush()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        class_encoded = {\n",
    "            0: 'not_related',\n",
    "            1: 'related'\n",
    "        }\n",
    "        val_true = np.array(val_labels).astype(int)[:-(val_len%batch_size)]\n",
    "        val_pred = []\n",
    "        for batch in batched_pairs(val_rlts, batch_size, dataset_period):\n",
    "            val_pred.append(self.model.predict(batch))\n",
    "        \n",
    "        val_pred = np.concatenate(val_pred, axis=0)\n",
    "        val_pos_m, val_pos_s, val_neg_m, val_neg_s = val_distance_stats(val_pred, val_true)\n",
    "        threshold = (val_pos_m + val_neg_m ) / 2\n",
    "        \n",
    "        # Precision and recall\n",
    "        val_pred = (val_pred.squeeze() < threshold).astype(int)\n",
    "        valid_precision, valid_recall, _, _ = precision_recall_fscore_support(val_true, val_pred, labels=[0, 1])\n",
    "        valid_accuracy = accuracy_score(val_true, val_pred)\n",
    "        \n",
    "        train_loss = logs['loss']\n",
    "        valid_loss = logs['val_loss']\n",
    "        logs = {}\n",
    "        logs['train/loss'] = train_loss\n",
    "        \n",
    "        self.tb_writer(logs, wtype='train')\n",
    "        \n",
    "        logs = {}\n",
    "        logs['valid/loss'] = valid_loss\n",
    "        \n",
    "        for k, v in class_encoded.items():\n",
    "            logs['valid/precision/' + v] = valid_precision[k]\n",
    "            logs['valid/recall/' + v] = valid_recall[k]\n",
    "            logs['valid/dist_mean/' + v] = val_pos_m if k else val_neg_m\n",
    "            logs['valid/dist_std/' + v] = val_pos_s if k else val_neg_s\n",
    "        \n",
    "        logs['valid/accuracy'] = valid_accuracy\n",
    "\n",
    "        self.tb_writer(logs, wtype='valid')\n",
    "        self.step_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobile_007_euc'\n",
    "\n",
    "#Save training configuration\n",
    "with open(f'configs/{model_name}.json', 'w') as f:\n",
    "    json.dump(train_config, f)\n",
    "\n",
    "logdir = os.path.join('logs', model_name)\n",
    "ckpt_dir = os.path.join('checkpoints', model_name)\n",
    "os.makedirs(ckpt_dir)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "ckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(ckpt_dir, 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    save_weights_only=True,\n",
    "    period=5\n",
    ")\n",
    "metric_callback = MetricCallback(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def repeat_generator(rlts, batch_size, dataset_period):\n",
    "    while True:\n",
    "        for e in batched_pairs(rlts, batch_size, dataset_period):\n",
    "            yield e\n",
    "            \n",
    "model.fit(\n",
    "    repeat_generator(train_rlts, batch_size, dataset_period),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_len//batch_size,\n",
    "    validation_data=repeat_generator(val_rlts, batch_size, dataset_period),\n",
    "    validation_steps=val_len//batch_size,\n",
    "    callbacks=[metric_callback, ckpt_callback]#, lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission pairs\n",
    "submission_path = 'data/sample_submission.csv'\n",
    "submission_df = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "ckpt_path = 'checkpoints/model_6/weights.70-0.11.hdf5'\n",
    "model.load_weights(ckpt_path)\n",
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the threshold according to validation ds\n",
    "val_pred = model.predict([val_pairs[:, 0], val_pairs[:, 1]])\n",
    "val_pos_m, val_pos_s, val_neg_m, val_neg_s = val_distance_stats(val_pred, val_labels.astype(np.int))\n",
    "threshold = ((val_pos_m + val_pos_s) + (val_neg_m - val_neg_s)) / 2\n",
    "\n",
    "# Iterate over submission pairs\n",
    "is_related = submission_df['is_related']\n",
    "predictions = []\n",
    "for idx, row in submission_df.iterrows():\n",
    "    # Load images\n",
    "    img_pair = row['img_pair']\n",
    "    img1_name, img2_name = img_pair.split('-')\n",
    "    img1_path = os.path.join('data/test', img1_name)\n",
    "    img2_path = os.path.join('data/test', img2_name)\n",
    "    img1 = image.load_img(img1_path)\n",
    "    img2 = image.load_img(img2_path)\n",
    "    img1 = np.array(img1).astype('float32')\n",
    "    img2 = np.array(img2).astype('float32')\n",
    "    \n",
    "    # Get FaceNet embeddings\n",
    "    embedding1 = embedder.embeddings([img1])\n",
    "    embedding2 = embedder.embeddings([img2])\n",
    "    \n",
    "    # Do an inference, if distance is smaller than threshold\n",
    "    # then there is the relation\n",
    "    y_pred = model.predict([embedding1, embedding2])\n",
    "    predictions.append(y_pred[0])\n",
    "    if y_pred.squeeze() < threshold:\n",
    "        is_related[idx] = 1\n",
    "    \n",
    "    # Print step\n",
    "    if idx % 100 == 0:\n",
    "        print(f'Processed rows: {idx}')\n",
    "        \n",
    "submission_df.to_csv(f'submission_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.85\n",
    "for i, p in enumerate(predictions):\n",
    "    if p < thr:\n",
    "        is_related[i] = 1\n",
    "    else:\n",
    "        is_related[i] = 0\n",
    "submission_df.to_csv(f'submission_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
