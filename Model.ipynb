{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv1D, Attention, GlobalAveragePooling1D\n",
    "from keras_facenet import FaceNet\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input embeddings\n",
    "\n",
    "The data in the input pickle file is stored in a dictionary structure:\n",
    "```\n",
    "{\n",
    "    [\n",
    "        'FAMILY_ID/PERSON_ID': [EMB_1, EMB_2...EMB_N],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_img_embeddings.pkl', 'rb') as f:\n",
    "       train_embeddings = pickle.load(f)\n",
    "print(f'The keys examples: {list(train_embeddings.keys())[:5]}')\n",
    "\n",
    "embedding_shape = list(train_embeddings.values())[0][0].shape\n",
    "print(f'Embeddings shape: {embedding_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1.0\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Compute classification accuracy with a fixed threshold on distances.\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Available training pairs from csv files are splitted to train - validation sets. Those pairs are positive(there is blood relation). For each set(train/valid) we additionally generate negative pairs.\n",
    "\n",
    "Positive pairs are generated according to the input csv file. For each person of positive pair we create one negative pair.\n",
    "In total we'll have twice more negative than positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_paris(train_rltshps):\n",
    "    '''\n",
    "    Create negative pairs: for each person of positive pair create negative pair\n",
    "    by picking some random person with whome they are not in the relationship.\n",
    "    '''\n",
    "    def if_pair_exists(df, anchor, new_sample):\n",
    "        '''\n",
    "        Check if a pair exists in the dataframe.\n",
    "        '''\n",
    "        pair_exists = ((df['p1'] == anchor) & (df['p2'] == new_sample)).any() \\\n",
    "                            or ((df['p1'] == new_sample) & (df['p2'] == anchor)).any()\n",
    "        return pair_exists\n",
    "    \n",
    "    all_persons = train_rltshps['p1'].unique().tolist() + \\\n",
    "                    train_rltshps['p2'].unique().tolist()\n",
    "    n = len(all_persons)\n",
    "    negative_rltchps = {'p1':[], 'p2':[]}\n",
    "    \n",
    "    for idx, row in train_rltshps.iterrows():\n",
    "        # Add negative pairs\n",
    "        # For the person p1\n",
    "        rnd_idx = np.random.randint(n)\n",
    "        negative_sample = all_persons[rnd_idx]\n",
    "\n",
    "        while(if_pair_exists(train_rltshps, row['p1'], negative_sample)):\n",
    "            rnd_idx = np.random.randint(n)\n",
    "            negative_sample = all_persons[rnd_idx]\n",
    "\n",
    "        negative_rltchps['p1'].append(row['p1'])\n",
    "        negative_rltchps['p2'].append(negative_sample)\n",
    "\n",
    "        # For the person p2\n",
    "        rnd_idx = np.random.randint(n)\n",
    "        negative_sample = all_persons[rnd_idx]\n",
    "\n",
    "        while(if_pair_exists(train_rltshps, row['p2'], negative_sample)):\n",
    "            rnd_idx = np.random.randint(n)\n",
    "            negative_sample = all_persons[rnd_idx]\n",
    "\n",
    "        negative_rltchps['p1'].append(negative_sample)\n",
    "        negative_rltchps['p2'].append(row['p2'])\n",
    "    \n",
    "    return pd.DataFrame(negative_rltchps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs, positive=True):\n",
    "    '''\n",
    "    Generate pairs of images of persons.\n",
    "    \n",
    "    Arguments:\n",
    "    input_pairs -- pandas DataFrame with pair paths\n",
    "    positive -- if pair is positive (persons are related)\n",
    "    \n",
    "    Returns:\n",
    "    train_pairs -- array of pairs of embeddings\n",
    "    labels -- labels for each pair, 1 for positive(in blood relation) and 0 for negative\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    errors = 0\n",
    "    for idx, row in input_pairs.iterrows():\n",
    "        try:\n",
    "            # Add positive pair\n",
    "            new_pairs = make_pairs(row['p1'], row['p2'])\n",
    "            pairs += new_pairs\n",
    "            label = 1. if positive else 0.\n",
    "            labels += [label] * len(new_pairs)\n",
    "            \n",
    "        except KeyError:\n",
    "            errors += 1\n",
    "    print(f'\\nThere are {errors} key errors of relationships.')\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(p1, p2):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    '''\n",
    "    pairs = []\n",
    "    img_path1 = p1.replace('/', '\\\\')\n",
    "    img_path2 = p2.replace('/', '\\\\')\n",
    "    \n",
    "    dir1 = np.expand_dims(train_embeddings[img_path1], axis=-1)\n",
    "    dir2 = np.expand_dims(train_embeddings[img_path2], axis=-1)\n",
    "    n = min(len(dir1), len(dir2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        pairs.append([dir1[i], dir2[i]])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relatives' pairs\n",
    "train_rltshps = pd.read_csv(\"data/train_relationships.csv\")\n",
    "train_rltshps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative relationships\n",
    "negative_rltshps = create_negative_paris(train_rltshps)\n",
    "negative_rltshps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows in pandas DataFrame\n",
    "train_rltshps = train_rltshps.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "negative_rltshps = negative_rltshps.sample(frac=1, random_state=123).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "# Split positive pairs\n",
    "VAL_FACTOR = 0.12\n",
    "val_threshold = int(len(train_rltshps.index) * VAL_FACTOR)\n",
    "val_rltshps = train_rltshps.iloc[:val_threshold]\n",
    "train_rltshps = train_rltshps.iloc[val_threshold:]\n",
    "print(train_rltshps.shape)\n",
    "print(val_rltshps.shape)\n",
    "\n",
    "# Split negative pairs\n",
    "val_threshold = int(len(negative_rltshps.index) * VAL_FACTOR)\n",
    "val_neg_rltshps = negative_rltshps.iloc[:val_threshold]\n",
    "train_neg_rltshps = negative_rltshps.iloc[val_threshold:]\n",
    "print(train_neg_rltshps.shape)\n",
    "print(val_neg_rltshps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, train_labels = pairs_set(train_rltshps, True)\n",
    "val_pairs, val_labels = pairs_set(val_rltshps, True)\n",
    "\n",
    "train_neg_pairs, train_neg_labels = pairs_set(train_neg_rltshps, False)\n",
    "val_neg_pairs, val_neg_labels = pairs_set(val_neg_rltshps, False)\n",
    "\n",
    "train_pairs = np.array(train_pairs + train_neg_pairs)\n",
    "train_labels = np.array(train_labels + train_neg_labels)\n",
    "val_pairs = np.array(val_pairs + val_neg_pairs)\n",
    "val_labels = np.array(val_labels + val_neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pairs.shape)\n",
    "print(val_pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamise network\n",
    "\n",
    "Initial experimenting is done with conv1D deep neural network, as additional option for experimenting there is simple attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1D_model(input_shape, l2_value, dropout):\n",
    "    '''\n",
    "    Create deep Keras model.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input layer\n",
    "    \n",
    "    Returns:\n",
    "    Model -- Keras model\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv1D(input.shape[1] // 64, 7, kernel_regularizer=l2(l2_value), activation='relu')(input)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 64, 11, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 32, 17, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 32, 17, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 16, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(input.shape[1] // 8, 19, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(input.shape[1] // 16, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(input.shape[1] // 32, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(input.shape[1] // 32, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def attention_model(input_shape, train_mode=True):\n",
    "    '''\n",
    "    Inspired by code example:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "    '''\n",
    "    input = Input(shape=input_shape, dtype='int32')\n",
    "    query_input = value_input = K.squeeze(input, -1)\n",
    "    \n",
    "    # Embedding lookup.\n",
    "    token_embedding = tf.keras.layers.Embedding(input_dim=input_shape[1], output_dim=64)\n",
    "    # Query embeddings of shape [batch_size, Tq, dimension].\n",
    "    query_embeddings = token_embedding(query_input)\n",
    "    # Value embeddings of shape [batch_size, Tv, dimension].\n",
    "    value_embeddings = token_embedding(value_input)\n",
    "\n",
    "    query_seq_encoding = Conv1D(input.shape[1] // 4, 5, activation='relu', padding='same')(\n",
    "        query_embeddings)\n",
    "    value_seq_encoding = Conv1D(input.shape[1] // 4, 5, activation='relu', padding='same')(\n",
    "        value_embeddings)\n",
    "    \n",
    "    query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "        [query_seq_encoding, value_seq_encoding], training=train_mode)\n",
    "    \n",
    "    # Reduce over the sequence axis to produce encodings of shape\n",
    "    # [batch_size, filters].\n",
    "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_seq_encoding)\n",
    "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_value_attention_seq)\n",
    "    \n",
    "    # Concatenate query and document encodings to produce a DNN input layer.\n",
    "    attn_out_layer = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\n",
    "    return Model(input, attn_out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "l2_value = 1e-5\n",
    "dropout = 0\n",
    "epochs = 2000\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input has 512 embeddings\n",
    "base_network = conv1D_model(train_pairs.shape[-2:], l2_value, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Siamese network\n",
    "input1 = Input(shape=embedding_shape[0])\n",
    "input2 = Input(shape=embedding_shape[0])\n",
    "processed1 = base_network(input1)\n",
    "processed2 = base_network(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "model = Model([input1, input2], distance)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard plugin in order to track changes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs --port=7007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model_name, logs_dir='./logs'):\n",
    "        super(Callback, self).__init__()\n",
    "        logdir = os.path.join(logs_dir, model_name)\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        self.train_writer = tf.summary.create_file_writer(logdir + '/train')\n",
    "        self.valid_writer = tf.summary.create_file_writer(logdir + '/valid')\n",
    "        self.step_number = 0\n",
    "        \n",
    "    def tb_writer(self, items_to_write, wtype):\n",
    "        writer = self.train_writer if wtype == 'train' else self.valid_writer\n",
    "        \n",
    "        with writer.as_default():\n",
    "            for name, value in items_to_write.items():\n",
    "                tf.summary.scalar(name, value, self.step_number)\n",
    "            writer.flush()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        class_encoded = {\n",
    "            0: 'not_related',\n",
    "            1: 'related'\n",
    "        }\n",
    "        \n",
    "        train_pred = self.model.predict([train_pairs[:, 0], train_pairs[:, 1]])\n",
    "        train_pred = (train_pred.squeeze() < 1.0).astype(int)\n",
    "        train_true = train_labels.astype(int)\n",
    "        \n",
    "        val_pred = self.model.predict([val_pairs[:, 0], val_pairs[:, 1]])\n",
    "        val_pred = (val_pred.squeeze() < 1.0).astype(int)\n",
    "        val_true = val_labels.astype(int)\n",
    "        \n",
    "        train_accuracy = accuracy_score(train_true, train_pred)\n",
    "        valid_accuracy = accuracy_score(val_true, val_pred)\n",
    "        train_precision, train_recall, _, _ = precision_recall_fscore_support(train_true, train_pred, labels=[0, 1])\n",
    "        valid_precision, valid_recall, _, _ = precision_recall_fscore_support(val_true, val_pred, labels=[0, 1])\n",
    "        \n",
    "        train_loss = logs['loss']\n",
    "        valid_loss = logs['val_loss']\n",
    "        logs = {}\n",
    "        logs['train/loss'] = train_loss\n",
    "        logs['train/accuracy'] = train_accuracy\n",
    "        \n",
    "        for k, v in class_encoded.items():\n",
    "            logs['train/precision/' + v] = train_precision[k]\n",
    "            logs['train/recall/' + v] = train_recall[k]\n",
    "        \n",
    "        self.tb_writer(logs, wtype='train')\n",
    "        \n",
    "        logs = {}\n",
    "        logs['valid/loss'] = valid_loss\n",
    "        logs['valid/accuracy'] = valid_accuracy\n",
    "        \n",
    "        for k, v in class_encoded.items():\n",
    "            logs['valid/precision/' + v] = valid_precision[k]\n",
    "            logs['valid/recall/' + v] = valid_recall[k]\n",
    "\n",
    "        self.tb_writer(logs, wtype='valid')\n",
    "        self.step_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_1'\n",
    "logdir = os.path.join('logs', model_name)\n",
    "ckpt_dir = os.path.join('checkpoints', model_name)\n",
    "os.makedirs(ckpt_dir)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "chkpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(ckpt_dir, 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    save_weights_only=True,\n",
    "    period=10\n",
    ")\n",
    "metric_callback = MetricCallback(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_pairs[:, 0],\n",
    "           train_pairs[:, 1]],\n",
    "           train_labels,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels),\n",
    "           callbacks=[metric_callback, chkpt_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission pairs\n",
    "submission_path = 'data/sample_submission.csv'\n",
    "submission_df = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "ckpt_path = 'checkpoints/model_1/weights.350-0.11.hdf5'\n",
    "model.load_weights(ckpt_path)\n",
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over submission pairs\n",
    "is_related = submission_df['is_related']\n",
    "for idx, row in submission_df.iterrows():\n",
    "    # Load images\n",
    "    img_pair = row['img_pair']\n",
    "    img1_name, img2_name = img_pair.split('-')\n",
    "    img1_path = os.path.join('data/test', img1_name)\n",
    "    img2_path = os.path.join('data/test', img2_name)\n",
    "    img1 = image.load_img(img1_path)\n",
    "    img2 = image.load_img(img2_path)\n",
    "    img1 = np.array(img1).astype('float32')\n",
    "    img2 = np.array(img2).astype('float32')\n",
    "    \n",
    "    # Get FaceNet embeddings\n",
    "    embedding1 = embedder.embeddings([img1])\n",
    "    embedding2 = embedder.embeddings([img2])\n",
    "    \n",
    "    # Do an inference, if distance is smaller than margin=1.0 (from contrastive loss)\n",
    "    # then there is the relation\n",
    "    y_pred = model.predict([embedding1, embedding2])\n",
    "    if y_pred.squeeze() < 1.0:\n",
    "        is_related[idx] = 1\n",
    "    \n",
    "    # Print step\n",
    "    if idx % 100 == 0:\n",
    "        print(f'Processed rows: {idx}')\n",
    "submission_df.to_csv(f'submission_{model_name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
