{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input embeddings\n",
    "\n",
    "The data in the input pickle file is stored in a dictionary structure:\n",
    "```\n",
    "{\n",
    "    [\n",
    "        'FAMILY_ID/PERSON_ID': [EMB_1, EMB_2...EMB_N],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys examples: ['F0002\\\\MID1', 'F0002\\\\MID2', 'F0002\\\\MID3', 'F0005\\\\MID1', 'F0005\\\\MID2']\n",
      "Embeddings shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "with open('data/train_img_embeddings.pkl', 'rb') as f:\n",
    "       train_embeddings = pickle.load(f)\n",
    "print(f'The keys examples: {list(train_embeddings.keys())[:5]}')\n",
    "\n",
    "embedding_shape = list(train_embeddings.values())[0][0].shape\n",
    "print(f'Embeddings shape: {embedding_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1.0\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    # Compute classification accuracy with a fixed threshold on distances.\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Compute classification accuracy with a fixed threshold on distances.\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Positive pairs are generated according to the input csv file, and negative pairs are generated mathcing persons from random differnet families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs):\n",
    "    '''\n",
    "    Create positive pairs according to input .csv file.\n",
    "    Negative pairs are generated by randomly picking people from different families.\n",
    "    \n",
    "    Arguments:\n",
    "    input_pairs -- pandas DataFrame with positive pairs paths\n",
    "    \n",
    "    Returns:\n",
    "    train_pairs -- array of pairs of embeddings\n",
    "    labels -- labels for each pair, 1 for positive(in blood relation) and 0 for negative\n",
    "    '''\n",
    "    n = len(input_pairs.index)\n",
    "    train_pairs = []\n",
    "    labels = []\n",
    "    errors = 0\n",
    "    for idx, row in input_pairs.iterrows():\n",
    "        try:\n",
    "            #Add positive pair\n",
    "            new_pairs = make_pairs(row['p1'], row['p2'])\n",
    "            train_pairs += new_pairs\n",
    "            labels += [1.] * len(new_pairs)\n",
    "            \n",
    "            #Add negative pair\n",
    "            rnd_idx = np.random.randint(n)\n",
    "            while(row['p1'][:5] == input_pairs.iloc[rnd_idx][1][:5]):\n",
    "                rnd_idx = np.random.randint(n)\n",
    "            \n",
    "            new_pairs = make_pairs(row['p1'], input_pairs.iloc[rnd_idx][1])\n",
    "            train_pairs += new_pairs\n",
    "            labels += [0.] * len(new_pairs)\n",
    "            \n",
    "        except KeyError:\n",
    "            errors += 1\n",
    "    print(f'\\nThere are {errors} key errors of {len(train_rltshps)} relationships.')\n",
    "    return np.array(train_pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(p1, p2):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    '''\n",
    "    pairs = []\n",
    "    img_path1 = p1.replace('/', '\\\\')\n",
    "    img_path2 = p2.replace('/', '\\\\')\n",
    "    \n",
    "    dir1 = np.expand_dims(train_embeddings[img_path1], axis=-1)\n",
    "    dir2 = np.expand_dims(train_embeddings[img_path2], axis=-1)\n",
    "    n = min(len(dir1), len(dir2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        pairs.append([dir1[i], dir2[i]])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F0002/MID1</td>\n",
       "      <td>F0002/MID3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F0002/MID2</td>\n",
       "      <td>F0002/MID3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F0005/MID1</td>\n",
       "      <td>F0005/MID2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0005/MID3</td>\n",
       "      <td>F0005/MID2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F0009/MID1</td>\n",
       "      <td>F0009/MID4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p1          p2\n",
       "0  F0002/MID1  F0002/MID3\n",
       "1  F0002/MID2  F0002/MID3\n",
       "2  F0005/MID1  F0005/MID2\n",
       "3  F0005/MID3  F0005/MID2\n",
       "4  F0009/MID1  F0009/MID4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read relatives' pairs\n",
    "train_rltshps = pd.read_csv(\"data/train_relationships.csv\")\n",
    "train_rltshps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of               p1           p2\n",
       "0     F0020/MID5  F0020/MID10\n",
       "1     F0123/MID3   F0123/MID4\n",
       "2     F0993/MID6   F0993/MID4\n",
       "3     F0101/MID8  F0101/MID12\n",
       "4     F0538/MID5   F0538/MID2\n",
       "...          ...          ...\n",
       "3593  F0198/MID5   F0198/MID2\n",
       "3594  F0579/MID1   F0579/MID7\n",
       "3595  F0064/MID2   F0064/MID5\n",
       "3596  F0916/MID1   F0916/MID2\n",
       "3597  F0818/MID2   F0818/MID6\n",
       "\n",
       "[3598 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle rows in pandas DataFrame\n",
    "train_rltshps = train_rltshps.sample(frac=1).reset_index(drop=True)\n",
    "train_rltshps.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3300, 2)\n",
      "(298, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation sets\n",
    "val_rltshps = train_rltshps.iloc[3300:]\n",
    "train_rltshps = train_rltshps.iloc[:3300]\n",
    "print(train_rltshps.shape)\n",
    "print(val_rltshps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 334 key errors of 3300 relationships.\n",
      "\n",
      "There are 25 key errors of 3300 relationships.\n"
     ]
    }
   ],
   "source": [
    "train_pairs, train_labels = pairs_set(train_rltshps)\n",
    "val_pairs, val_labels = pairs_set(val_rltshps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19972, 2, 512, 1)\n",
      "(1768, 2, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs.shape)\n",
    "print(val_pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    '''\n",
    "    Create deep Keras model.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input layer\n",
    "    \n",
    "    Returns:\n",
    "    Model -- Keras model\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv1D(input.shape[1] // 2, 3, activation='relu')(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(input.shape[1] // 4, 5, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(input.shape[1] // 8, 5, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(input.shape[1] // 4, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 4, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 4, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 8, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 8, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 8, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 8, activation='relu')(x)\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input has 512 embeddings\n",
    "base_network = create_base_network(train_pairs.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 510, 256)          1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 510, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 506, 128)          163968    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 506, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 502, 64)           41024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 502, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32128)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4112512   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 4,372,288\n",
      "Trainable params: 4,372,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Siamese network\n",
    "input1 = Input(shape=embedding_shape[0])\n",
    "input2 = Input(shape=embedding_shape[0])\n",
    "processed1 = base_network(input1)\n",
    "processed2 = base_network(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "model = Model([input1, input2], distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 2000\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard plugin in order to track changes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join('logs', 'model_1')\n",
    "ckpt_dir = os.path.join('checkpoints', 'model_1')\n",
    "os.makedirs(ckpt_dir)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "chkpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(ckpt_dir, 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6cccd12b472d52eb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6cccd12b472d52eb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 7007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./logs --port=7007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "   1/1249 [..............................] - ETA: 0s - loss: 0.4353WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1249/1249 [==============================] - 135s 108ms/step - loss: 0.2592 - val_loss: 0.4489\n",
      "Epoch 2/2000\n",
      "1249/1249 [==============================] - 133s 106ms/step - loss: 0.2555 - val_loss: 0.3185\n",
      "Epoch 3/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2564 - val_loss: 0.4584\n",
      "Epoch 4/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2547 - val_loss: 0.4584\n",
      "Epoch 5/2000\n",
      "1249/1249 [==============================] - 129s 104ms/step - loss: 0.2550 - val_loss: 0.4584\n",
      "Epoch 6/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2558 - val_loss: 0.4584\n",
      "Epoch 7/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2569 - val_loss: 0.4584\n",
      "Epoch 8/2000\n",
      "1249/1249 [==============================] - 129s 104ms/step - loss: 0.2548 - val_loss: 0.4584\n",
      "Epoch 9/2000\n",
      "1249/1249 [==============================] - 129s 104ms/step - loss: 0.2563 - val_loss: 0.4584\n",
      "Epoch 10/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2551 - val_loss: 0.4584\n",
      "Epoch 11/2000\n",
      "1249/1249 [==============================] - 131s 105ms/step - loss: 0.2557 - val_loss: 0.4584\n",
      "Epoch 12/2000\n",
      "1249/1249 [==============================] - 129s 104ms/step - loss: 0.2555 - val_loss: 0.4584\n",
      "Epoch 13/2000\n",
      "1249/1249 [==============================] - 129s 104ms/step - loss: 0.2558 - val_loss: 0.4584\n",
      "Epoch 14/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2549 - val_loss: 0.4584\n",
      "Epoch 15/2000\n",
      "1249/1249 [==============================] - 132s 106ms/step - loss: 0.2548 - val_loss: 0.4584\n",
      "Epoch 16/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2552 - val_loss: 0.4584\n",
      "Epoch 17/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2552 - val_loss: 0.4584\n",
      "Epoch 18/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2557 - val_loss: 0.4584\n",
      "Epoch 19/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2558 - val_loss: 0.4584\n",
      "Epoch 20/2000\n",
      "1249/1249 [==============================] - 131s 104ms/step - loss: 0.2562 - val_loss: 0.4584\n",
      "Epoch 21/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2543 - val_loss: 0.4584\n",
      "Epoch 22/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2543 - val_loss: 0.4584\n",
      "Epoch 23/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2544 - val_loss: 0.4584\n",
      "Epoch 24/2000\n",
      "1249/1249 [==============================] - 131s 105ms/step - loss: 0.2546 - val_loss: 0.4584\n",
      "Epoch 25/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2548 - val_loss: 0.4584\n",
      "Epoch 26/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2560 - val_loss: 0.4584\n",
      "Epoch 27/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2550 - val_loss: 0.4584\n",
      "Epoch 28/2000\n",
      "1249/1249 [==============================] - 131s 105ms/step - loss: 0.2561 - val_loss: 0.4584\n",
      "Epoch 29/2000\n",
      "1249/1249 [==============================] - 130s 104ms/step - loss: 0.2544 - val_loss: 0.4584\n",
      "Epoch 30/2000\n",
      " 108/1249 [=>............................] - ETA: 2:05 - loss: 0.2522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d8c03faa004c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchkpt_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m          )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([train_pairs[:, 0],\n",
    "           train_pairs[:, 1]],\n",
    "           train_labels,\n",
    "           batch_size=16,\n",
    "           epochs=epochs,\n",
    "           validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels),\n",
    "           callbacks=[tensorboard_callback, chkpt_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test accuracy\n",
    "model_ckpt = ''\n",
    "model.load_weigths(model_ckpt)\n",
    "y_pred = model.predict([train_pairs[:, 0], train_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(train_labels, y_pred < 1.0)\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r logs/model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'checkpoints/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r checkpoints/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(y_pred, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
