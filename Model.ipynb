{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input embeddings\n",
    "\n",
    "The data in the input pickle file is stored in a dictionary structure:\n",
    "```\n",
    "{\n",
    "    [\n",
    "        'FAMILY_ID/PERSON_ID': [EMB_1, EMB_2...EMB_N],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_img_embeddings.pkl', 'rb') as f:\n",
    "       train_embeddings = pickle.load(f)\n",
    "print(f'The keys examples: {list(train_embeddings.keys())[:5]}')\n",
    "\n",
    "embedding_shape = list(train_embeddings.values())[0][0].shape\n",
    "print(f'Embeddings shape: {embedding_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    # Compute classification accuracy with a fixed threshold on distances.\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Compute classification accuracy with a fixed threshold on distances.\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Positive pairs are generated according to the input csv file, and negative pairs are generated mathcing persons from random differnet families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs):\n",
    "    '''\n",
    "    Create positive pairs according to input .csv file.\n",
    "    Negative pairs are generated by randomly picking people from different families.\n",
    "    \n",
    "    Arguments:\n",
    "    input_pairs -- pandas DataFrame with positive pairs paths\n",
    "    \n",
    "    Returns:\n",
    "    train_pairs -- array of pairs of embeddings\n",
    "    labels -- labels for each pair, 1 for positive(in blood relation) and 0 for negative\n",
    "    '''\n",
    "    n = len(input_pairs.index)\n",
    "    train_pairs = []\n",
    "    labels = []\n",
    "    errors = 0\n",
    "    for idx, row in input_pairs.iterrows():\n",
    "        try:\n",
    "            #Add positive pair\n",
    "            new_pairs = make_pairs(row['p1'], row['p2'])\n",
    "            train_pairs += new_pairs\n",
    "            labels += [1.] * len(new_pairs)\n",
    "            \n",
    "            #Add negative pair\n",
    "            rnd_idx = np.random.randint(n)\n",
    "            while(row['p1'][:5] == input_pairs.iloc[rnd_idx][1][:5]):\n",
    "                rnd_idx = np.random.randint(n)\n",
    "            \n",
    "            new_pairs = make_pairs(row['p1'], input_pairs.iloc[rnd_idx][1])\n",
    "            train_pairs += new_pairs\n",
    "            labels += [0.] * len(new_pairs)\n",
    "            \n",
    "        except KeyError:\n",
    "            errors += 1\n",
    "    print(f'\\nThere are {errors} key errors of {len(train_rltshps)} relationships.')\n",
    "    return np.array(train_pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(p1, p2):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    '''\n",
    "    pairs = []\n",
    "    img_path1 = p1.replace('/', '\\\\')\n",
    "    img_path2 = p2.replace('/', '\\\\')\n",
    "    \n",
    "    dir1 = train_embeddings[img_path1]\n",
    "    dir2 = train_embeddings[img_path2]\n",
    "    n = min(len(dir1), len(dir2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        pairs.append([dir1[i], dir2[i]])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relatives' pairs\n",
    "train_rltshps = pd.read_csv(\"data/train_relationships.csv\")\n",
    "train_rltshps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows in pandas DataFrame\n",
    "train_rltshps = train_rltshps.sample(frac=1).reset_index(drop=True)\n",
    "train_rltshps.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "val_rltshps = train_rltshps.iloc[3300:]\n",
    "train_rltshps = train_rltshps.iloc[:3300]\n",
    "print(train_rltshps.shape)\n",
    "print(val_rltshps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, train_labels = pairs_set(train_rltshps)\n",
    "val_pairs, val_labels = pairs_set(val_rltshps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pairs.shape)\n",
    "print(val_pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    '''\n",
    "    Create deep Keras model.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input layer\n",
    "    \n",
    "    Returns:\n",
    "    Model -- Keras model\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Dense(input.shape[1] // 2, activation = 'relu')(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 4, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(input.shape[1] // 8, activation='relu')(x)\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input has 512 embeddings\n",
    "base_network = create_base_network(embedding_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Siamese network\n",
    "input1 = Input(shape=embedding_shape[0])\n",
    "input2 = Input(shape=embedding_shape[0])\n",
    "processed1 = base_network(input1)\n",
    "processed2 = base_network(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "\n",
    "model = Model([input1, input2], distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 2000\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard plugin in order to track changes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", 'model_2')\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs --port=7007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_pairs[:, 0],\n",
    "           train_pairs[:, 1]],\n",
    "           train_labels,\n",
    "           batch_size=128,\n",
    "           epochs=epochs,\n",
    "           validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels),\n",
    "           callbacks=[tensorboard_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test accuracy\n",
    "y_pred = model.predict([val_pairs[:, 0], val_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(val_labels, y_pred < 1.)\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
