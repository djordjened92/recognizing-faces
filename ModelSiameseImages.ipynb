{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow import keras\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.applications import MobileNet, ResNet50, InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from keras_vggface.utils import preprocess_input as vggface_preprocess\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, \\\n",
    "Conv1D, Attention, GlobalAveragePooling1D, BatchNormalization, Concatenate, \\\n",
    "Layer, Reshape, Add, Multiply, Subtract\n",
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "random.seed(123)\n",
    "tf.random.set_seed(12)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Available training pairs from csv files are splitted to train - validation sets. Those pairs are positive(there is blood relation). For each set(train/valid) we additionally generate negative pairs.\n",
    "\n",
    "Positive pairs are generated according to the input csv file. For each person of positive pair we create one negative pair.\n",
    "In total we'll have twice more negative than positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_pair(pair, input_shape, shuffle=True, slice_imgs=None):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    ''' \n",
    "    p1, p2 = [os.path.join(TRAIN_PATH, p) for p in pair]\n",
    "    \n",
    "    p1_imgs = os.listdir(p1)\n",
    "    p2_imgs = os.listdir(p2)\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(p1_imgs)\n",
    "        random.shuffle(p2_imgs)\n",
    "    \n",
    "    p1_imgs = p1_imgs[:slice_imgs]\n",
    "    p2_imgs = p2_imgs[:slice_imgs]\n",
    "    \n",
    "    for i in range(len(p1_imgs)):\n",
    "        for j in range(len(p2_imgs)):\n",
    "            img1_path = os.path.join(p1, p1_imgs[i])\n",
    "            img2_path = os.path.join(p2, p2_imgs[j])\n",
    "            img1 = image.load_img(img1_path, target_size=(input_shape[0], input_shape[1]))\n",
    "            img2 = image.load_img(img2_path, target_size=(input_shape[0], input_shape[1]))\n",
    "            img1 = np.array(img1).astype('float32')\n",
    "            img2 = np.array(img2).astype('float32')\n",
    "            \n",
    "            yield img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs, input_shape, shuffle=True, slice_imgs=None):\n",
    "    for pair, label in input_pairs:\n",
    "        try:\n",
    "            img_pairs = make_image_pair(pair, input_shape, shuffle, slice_imgs)\n",
    "            for img_pair in img_pairs:\n",
    "                yield img_pair, label\n",
    "        except (KeyError, FileNotFoundError):\n",
    "            continue\n",
    "\n",
    "def batched_pairs(input_pairs, batch_size, dataset_period, input_shape, preprocess, shuffle=True, slice_imgs=None):\n",
    "    imgs1 = []\n",
    "    imgs2 = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    for example in pairs_set(input_pairs, input_shape, shuffle, slice_imgs):\n",
    "        # Get every nth sample\n",
    "        counter += 1\n",
    "        if counter % dataset_period:\n",
    "            continue\n",
    "        \n",
    "        exmpls, label = example\n",
    "        exmpl1, exmpl2 = exmpls\n",
    "        imgs1.append(exmpl1)\n",
    "        imgs2.append(exmpl2)\n",
    "        labels.append(label)\n",
    "        if len(labels) == batch_size:\n",
    "            yield {'input_1':preprocess(np.array(imgs1)), 'input_2':preprocess(np.array(imgs2))}, np.array(labels).astype(float)\n",
    "            imgs1, imgs2, labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_val_set.json', 'r') as f:\n",
    "    train_val_set = json.load(f)\n",
    "\n",
    "train_rlt_list, neg_train_rltshps, valid_rlt_list, neg_valid_rltshps = list(train_val_set.values())\n",
    "train_rlt_list = train_rlt_list * 4\n",
    "\n",
    "train_rlts = list(zip(train_rlt_list + neg_train_rltshps, [True]*len(train_rlt_list) + [False]*len(neg_train_rltshps)))\n",
    "val_rlts = list(zip(valid_rlt_list + neg_valid_rltshps, [True]*len(valid_rlt_list) + [False]*len(neg_valid_rltshps)))\n",
    "\n",
    "random.shuffle(train_rlts)\n",
    "random.shuffle(val_rlts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network\n",
    "\n",
    "Initial experimenting is done with conv1D deep neural network, as additional option for experimenting there is simple attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y\n",
    "\n",
    "def cross_correlation(vectors):\n",
    "    '''\n",
    "    The goal is to convolute outputs from both networks,\n",
    "    each one from the batch of the first network output\n",
    "    over appropriate one from another network output\n",
    "    '''\n",
    "    \n",
    "    # x and y have shape: [batch_size, vector_size]\n",
    "    x, y = vectors\n",
    "    # We need to add 'channels' dimension -> [batch_size, vector_size, 1]\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    \n",
    "    # Firstly we do convolution as the second network\n",
    "    # output vectors from the batch are all filters.\n",
    "    # Technically, we've done convolution of all y over each x.\n",
    "    # From documentation(https://www.tensorflow.org/api_docs/python/tf/nn/conv1d) desired shapes should be:\n",
    "    # x shape: batch_shape(batch_size) + [in_width(vector_size), in_channels(1)]\n",
    "    # y shape: [filter_width(vector_size), in_channels(1), out_channels(batch_size)]\n",
    "    # Output shape is [batch_size, vector_length, batch_size].\n",
    "    conv1d = tf.nn.conv1d(x, tf.transpose(y, perm=[1, 2, 0]), stride=1, padding='SAME')\n",
    "    conv1d_shape = conv1d.shape\n",
    "    \n",
    "    # We need now to apply mask to the output in order to get desired\n",
    "    # convolution results in the shape [batch_size, vector_length]\n",
    "    bool_diag = tf.linalg.diag(tf.constant([True] * config.batch_size))\n",
    "    mask = tf.repeat(tf.expand_dims(bool_diag, axis=1), conv1d_shape[1], axis=1)\n",
    "    out = tf.squeeze(tf.ragged.boolean_mask(conv1d, mask), axis=-1)\n",
    "    return out.to_tensor()\n",
    "\n",
    "def mobilenet(input_shape, l2_value, dropout, model_name='mobilenet', trainable_after=None):\n",
    "    mobile = MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        dropout=dropout,\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        alpha=1.,\n",
    "        weights=None\n",
    "    )\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in mobile.layers:\n",
    "        trainable = trainable or layer.name.startswith(trainable_after)\n",
    "        layer.trainable = trainable\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = Dense(1024, kernel_regularizer=l2(l2_value), activation='relu')(mobile.output)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    return Model(mobile.input, x)\n",
    "\n",
    "def inception(input_shape, l2_value, dropout, model_name='inception', trainable_after=None):\n",
    "    inception = InceptionV3(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in inception.layers:\n",
    "        trainable = trainable or layer.name.startswith(trainable_after)\n",
    "        layer.trainable = trainable\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = Dense(128, kernel_regularizer=l2(l2_value), activation='relu')(inception.output)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    return Model(inception.input, x)\n",
    "\n",
    "def resnet50(input_shape, l2_value, dropout, model_name='resnet50', trainable_after=None):\n",
    "    resnet = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in resnet.layers:\n",
    "        trainable = trainable or layer.name.startswith(trainable_after)\n",
    "        layer.trainable = trainable\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        \n",
    "    x = Dense(32, kernel_regularizer=l2(l2_value), activation='relu')(resnet.output)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    return Model(resnet.input, x)\n",
    "\n",
    "def vggface_resnet50(input_shape, l2_value, dropout, model_name='vgg', trainable_after=None):\n",
    "    vggface_res = VGGFace(model='resnet50', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in vggface_res.layers:\n",
    "        trainable = trainable or layer.name.startswith(trainable_after)\n",
    "        layer.trainable = trainable\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        if isinstance(layer, Dropout):\n",
    "            layer.rate = dropout\n",
    "    \n",
    "    last_layer = vggface_res.get_layer('avg_pool').output\n",
    "    \n",
    "    # For independent instatiation\n",
    "    x = Reshape((-1, 1))(last_layer)\n",
    "    \n",
    "    x = Conv1D(1, 128)(x)\n",
    "    x = Conv1D(1, 128)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "\n",
    "    # For instatiation as part of hybrid model\n",
    "#     x = Flatten()(last_layer)\n",
    "#     x = Dense(256, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    \n",
    "    return Model(vggface_res.input, x, name=model_name)\n",
    "\n",
    "def facenet_inception_resnetv1(input_shape, l2_value, dropout, model_name='facenet', trainable_after=None):\n",
    "    inception_resnet = InceptionResNetV1(input_shape=input_shape, weights_path='keras-facenet/weights/facenet_keras_weights.h5')\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in inception_resnet.layers:\n",
    "        trainable = trainable or layer.name.startswith(trainable_after)\n",
    "        layer.trainable = trainable\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(l2_value))\n",
    "        if isinstance(layer, Dropout):\n",
    "            layer.rate = dropout\n",
    "    \n",
    "    last_layer = inception_resnet.get_layer('Dropout').output\n",
    "    \n",
    "    # For independent instatiation\n",
    "    x = Reshape((-1, 1))(last_layer)\n",
    "    \n",
    "    x = Conv1D(1, 128)(x)\n",
    "    x = Conv1D(1, 128)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    \n",
    "    # For instatiation as part of hybrid model\n",
    "#     x = Flatten()(last_layer)\n",
    "#     x = Dense(256, kernel_regularizer=l2(l2_value), activation='relu')(x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "# #     x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "    \n",
    "    return Model(inception_resnet.input, x, name=model_name)\n",
    "\n",
    "def vgg_facenet(input_shape, l2_value, dropout, model_name='hybrid', trainable_after=None):\n",
    "    vgg_model = vggface_resnet50(input_shape, l2_value, dropout, model_name='vgg', trainable_after=trainable_after)\n",
    "    vgg_model = Model(vgg_model.input, vgg_model.layers[-2].output, name='vgg_reduced')\n",
    "    facenet_model = facenet_inception_resnetv1(input_shape, l2_value, dropout, model_name='facenet', trainable_after=trainable_after)\n",
    "    facenet_model = Model(facenet_model.input, facenet_model.layers[-2].output, name='facenet_reduced')\n",
    "    \n",
    "    model_input = Input(input_shape)\n",
    "    vgg_out = vgg_model(model_input)\n",
    "    facenet_out = facenet_model(model_input)\n",
    "    \n",
    "#     multiply = Multiply()([vgg_out, facenet_out])\n",
    "#     multiply = Reshape((-1, 1))(multiply)\n",
    "#     multiply = Conv1D(1, 13, padding='same')(multiply)\n",
    "    \n",
    "    add = Add()([vgg_out, facenet_out])\n",
    "#     add = Reshape((-1, 1))(add)\n",
    "#     add = Conv1D(1, 13, padding='same')(add)\n",
    "    \n",
    "#     sub = Subtract()([vgg_out, facenet_out])\n",
    "#     sub_abs = Lambda(lambda x: K.abs(x))(sub)\n",
    "#     sub_abs = Reshape((-1, 1))(sub_abs)\n",
    "#     sub_abs = Conv1D(1, 13, padding='same')(sub_abs)\n",
    "    \n",
    "    output = Concatenate(axis=1)([vgg_out, facenet_out, add])#([multiply, add, sub_abs])\n",
    "#     output = Reshape((-1, 1))(output)\n",
    "    \n",
    "#     output = Conv1D(1, 13, padding='same')(output)\n",
    "#     output = Flatten()(output)\n",
    "    \n",
    "#     output = Dense(256, kernel_regularizer=l2(l2_value), activation='relu')(output)\n",
    "#     output = Dropout(dropout)(output)\n",
    "    output = Lambda(lambda x: K.l2_normalize(x,axis=1))(output)\n",
    "    \n",
    "    return Model(model_input, output, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        # A number of images per person for\n",
    "        # image-pair generation\n",
    "        self.train_slice_persons_images = 4\n",
    "        self.val_slice_persons_imges = 2\n",
    "        \n",
    "        # Factor for scaling variance loss in the total loss sum\n",
    "        self.var_loss = 0.\n",
    "        \n",
    "        # Margin offset for positive and negative margins\n",
    "        self.alpha = 0.2\n",
    "        \n",
    "        self.input_shape = (224, 224, 3)\n",
    "        \n",
    "        # Trainig parameters\n",
    "        self.learning_rate = 5e-5\n",
    "        self.l2_value = 1e-7\n",
    "        self.dropout = 0.1\n",
    "        self.epochs = 1000\n",
    "        self.batch_size = 16\n",
    "        self.model_type = 'mobilenet'\n",
    "        self.preprocess = 'mobilenet_preprocess'\n",
    "        \n",
    "        # Sampling period of generated pairs\n",
    "        self.train_dataset_period = 1\n",
    "        self.eval_dataset_period = 1\n",
    "        \n",
    "        # Available distance types:\n",
    "        # 'euclidean_distance', 'cosine_distance', 'cos_euc_distance'\n",
    "        self.distance_type = 'cosine_distance'\n",
    "        self.optimizer = 'Adam'\n",
    "        \n",
    "        # A layer name (or at least starting part of the name)\n",
    "        # after which all layers are trainable\n",
    "        self.trainable_after_layer = 'input'\n",
    "        \n",
    "        # Path to the pretrained model.\n",
    "        # If there is no any, than None\n",
    "        self.pretrained_base = 'pretrained/checkpoints/mobilenet_contrastive/model.hdf5'\n",
    "        \n",
    "config = Configuration()\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 10:\n",
    "        return 0.5 * lr\n",
    "    elif epoch == 180:\n",
    "        return 0.8 * lr\n",
    "    elif epoch == 250:\n",
    "        return 0.5 * lr\n",
    "    elif epoch == 300:\n",
    "        return 0.7 * lr\n",
    "    elif epoch == 400:\n",
    "        return 0.3 *lr\n",
    "    elif epoch == 700:\n",
    "        return 0.6 *lr\n",
    "    return lr\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin(model):\n",
    "    MARGIN = None\n",
    "    for weight in model.layers[-1].weights:\n",
    "        if weight.name == 'margin:0':\n",
    "            MARGIN = weight\n",
    "            break\n",
    "    \n",
    "    return MARGIN\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    sum_square = K.sum(K.square(x - y), axis=1)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def cosine_distance(vectors):\n",
    "    x, y = vectors\n",
    "    x_norm = tf.norm(x, axis=1)\n",
    "    y_norm = tf.norm(y, axis=1)\n",
    "    x_y_dot = tf.einsum('ij,ij->i', x, y)\n",
    "    cos_sim = x_y_dot / (x_norm * y_norm + K.epsilon())\n",
    "    return 1. - cos_sim\n",
    "\n",
    "def cos_euc_distance(vectors):\n",
    "    euc = euclidean_distance(vectors)\n",
    "    cos_dist = cosine_distance(vectors)\n",
    "    return cos_dist * euc\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "class ContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, model, var_loss_factor):\n",
    "        self.model = model\n",
    "        self.var_loss_factor = var_loss_factor\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        weight_pos = 1.\n",
    "        weight_neg = 1.\n",
    "        \n",
    "        y_true = tf.squeeze(y_true, [1]) if len(y_true.shape) == 2 else y_true\n",
    "        y_pred = tf.squeeze(y_pred, [1]) if len(y_pred.shape) == 2 else y_pred\n",
    "        \n",
    "        MARGIN = get_margin(self.model)\n",
    "        margin_pos = MARGIN - config.alpha\n",
    "        margin_neg = MARGIN + config.alpha\n",
    "\n",
    "        # Filter predictions\n",
    "        pred_pos = tf.boolean_mask(y_pred, y_true)\n",
    "        pred_neg = tf.boolean_mask(y_pred, 1 - y_true)\n",
    "        pred_pos_m = tf.reduce_mean(pred_pos, axis=0)\n",
    "        pred_neg_m = tf.reduce_mean(pred_neg, axis=0)\n",
    "\n",
    "        # Contrastive loss\n",
    "        square_pos = tf.square(tf.maximum(pred_pos - margin_pos, 0))\n",
    "        square_neg = tf.square(tf.maximum(margin_neg - pred_neg, 0))\n",
    "        squared_concat = tf.concat([weight_pos * square_pos, weight_neg * square_neg], axis=0)\n",
    "\n",
    "        # Variance loss\n",
    "        pred_pos = tf.boolean_mask(pred_pos, pred_pos > margin_pos)\n",
    "        pred_neg = tf.boolean_mask(pred_neg, pred_neg < margin_neg)\n",
    "        pred_pos_m = tf.reduce_mean(pred_pos, axis=0)\n",
    "        pred_neg_m = tf.reduce_mean(pred_neg, axis=0)\n",
    "        var_pos = 0. if tf.equal(tf.size(pred_pos), 0) else tf.reduce_mean(tf.square(pred_pos_m - pred_pos))\n",
    "        var_neg = 0. if tf.equal(tf.size(pred_neg), 0) else tf.reduce_mean(tf.square(pred_neg_m - pred_neg))\n",
    "        variance_loss = 0.5 * (var_pos + var_neg)\n",
    "\n",
    "\n",
    "        return tf.reduce_mean(squared_concat) + self.var_loss_factor * variance_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", seed=123),\n",
    "    keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.2, width_factor=0.2, seed=123),\n",
    "    keras.layers.experimental.preprocessing.RandomContrast(factor=0.2, seed=123),\n",
    "    keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, seed=123),\n",
    "    keras.layers.experimental.preprocessing.RandomRotation(factor=0.1, seed=123)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = eval(config.model_type)(\n",
    "    config.input_shape,\n",
    "    config.l2_value,\n",
    "    config.dropout,\n",
    "    'base_network',\n",
    "    trainable_after=config.trainable_after_layer\n",
    ")\n",
    "\n",
    "if config.pretrained_base:\n",
    "    base_network.load_weights(config.pretrained_base)\n",
    "\n",
    "print(f'Base network parameters count: {base_network.count_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Siamese network\n",
    "input1 = Input(shape=config.input_shape, name='input_1')\n",
    "input2 = Input(shape=config.input_shape, name='input_2')\n",
    "\n",
    "input_1_aug = data_augmentation(input1)\n",
    "input_2_aug = data_augmentation(input2)\n",
    "\n",
    "processed1 = base_network(input_1_aug)\n",
    "processed2 = base_network(input_2_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_function = eval(config.distance_type)\n",
    "distance = Lambda(dist_function,\n",
    "                  output_shape=eucl_dist_output_shape)([processed1, processed2])\n",
    "model = Model([input1, input2], distance)\n",
    "\n",
    "model.layers[len(model.layers) - 1].add_weight(\n",
    "    name = 'margin',\n",
    "    shape = (),\n",
    "    initializer = keras.initializers.constant(0.5),\n",
    "    trainable = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard plugin in order to track changes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./logs --port=7008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training set length\n",
    "train_len = 0\n",
    "for pair, label in train_rlts:\n",
    "    try:\n",
    "        p1, p2 = [os.path.join(TRAIN_PATH, p) for p in pair]\n",
    "        p1_imgs = os.listdir(p1)\n",
    "        p2_imgs = os.listdir(p2)\n",
    "        train_len += len(p1_imgs[:config.train_slice_persons_images]) * \\\n",
    "                     len(p2_imgs[:config.train_slice_persons_images])\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "train_len = train_len // config.train_dataset_period\n",
    "\n",
    "val_len = 0\n",
    "for pair, label in val_rlts:\n",
    "    try:\n",
    "        p1, p2 = [os.path.join(TRAIN_PATH, p) for p in pair]\n",
    "        p1_imgs = os.listdir(p1)\n",
    "        p2_imgs = os.listdir(p2)\n",
    "        val_len += len(p1_imgs[:config.val_slice_persons_imges]) * \\\n",
    "                     len(p2_imgs[:config.val_slice_persons_imges])\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "val_len = val_len // config.eval_dataset_period\n",
    "\n",
    "print(f'Train set length: {train_len}')\n",
    "print(f'Valid set length: {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_distance_stats(predictions, labels):\n",
    "    val_pos = predictions[labels.astype(np.bool)]\n",
    "    val_neg = predictions[(1 - labels).astype(np.bool)]\n",
    "    val_pos_m, val_pos_s = np.mean(val_pos), np.std(val_pos)\n",
    "    val_neg_m, val_neg_s = np.mean(val_neg), np.std(val_neg)\n",
    "    \n",
    "    return val_pos_m, val_pos_s, val_neg_m, val_neg_s\n",
    "   \n",
    "def dist_to_prob(predictions, lower_lim, upper_lim):\n",
    "    y_prob = 1 - (np.clip(predictions, lower_lim, upper_lim) - lower_lim) / (upper_lim - lower_lim)\n",
    "    return y_prob\n",
    "\n",
    "def get_limits(MARGIN):\n",
    "    # Get upper and lower boundary for the predicted distances\n",
    "    margin_pos = MARGIN - config.alpha\n",
    "    margin_neg = MARGIN + config.alpha\n",
    "    lower_lim = max(MARGIN - 2. * (MARGIN - margin_pos), 0.)\n",
    "    upper_lim = min(MARGIN + 2. * (margin_neg - MARGIN), 2.)\n",
    "    \n",
    "    return lower_lim, upper_lim\n",
    "    \n",
    "class MetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir):\n",
    "        super(MetricCallback, self).__init__()\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        self.train_writer = tf.summary.create_file_writer(logdir + '/train')\n",
    "        self.valid_writer = tf.summary.create_file_writer(logdir + '/valid')\n",
    "        self.class_encoded = {\n",
    "            0: 'not_related',\n",
    "            1: 'related'\n",
    "        }\n",
    "        \n",
    "    def tb_writer(self, items_to_write, wtype, epoch):\n",
    "        writer = self.train_writer if wtype == 'train' else self.valid_writer\n",
    "        \n",
    "        with writer.as_default():\n",
    "            for name, value in items_to_write.items():\n",
    "                tf.summary.scalar(name, value, epoch)\n",
    "            writer.flush()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Get current value of MARGIN\n",
    "        MARGIN = get_margin(self.model).numpy()\n",
    "        print(f'Current margin value: {MARGIN}')\n",
    "        \n",
    "        lower_lim, upper_lim = get_limits(MARGIN)\n",
    "        \n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        batches = batched_pairs(\n",
    "            val_rlts,\n",
    "            config.batch_size,\n",
    "            config.eval_dataset_period,\n",
    "            config.input_shape,\n",
    "            eval(config.preprocess),\n",
    "            shuffle=False,\n",
    "            slice_imgs=config.val_slice_persons_imges\n",
    "        )\n",
    "        \n",
    "        for batch in batches:\n",
    "            pred = self.model.predict(batch[0])\n",
    "            val_pred.append(pred)\n",
    "            val_true.extend(list(batch[1]))\n",
    "        \n",
    "        val_true = np.array(val_true)\n",
    "        val_pred = np.concatenate(val_pred, axis=0).squeeze()\n",
    "        val_loss = ContrastiveLoss(self.model, config.var_loss)(K.constant(val_true), K.constant(val_pred))\n",
    "        \n",
    "        val_true = val_true.astype(int)\n",
    "        val_pos_m, val_pos_s, val_neg_m, val_neg_s = val_distance_stats(val_pred, val_true)\n",
    "        threshold = MARGIN\n",
    "        \n",
    "        # Precision and recall\n",
    "        val_cls = (val_pred < threshold).astype(int)\n",
    "        val_precision, val_recall, _, _ = precision_recall_fscore_support(val_true, val_cls, labels=[0, 1])\n",
    "        val_accuracy = accuracy_score(val_true, val_cls)\n",
    "        \n",
    "        # Area under ROC\n",
    "        val_probs = dist_to_prob(val_pred, lower_lim, upper_lim)\n",
    "        val_roc_auc = roc_auc_score(val_true, val_probs)\n",
    "        train_loss = logs['loss']\n",
    "        tb_logs = {}\n",
    "        tb_logs['train/loss'] = train_loss\n",
    "        tb_logs['train/margin'] = MARGIN\n",
    "        self.tb_writer(tb_logs, wtype='train', epoch=epoch)\n",
    "        \n",
    "        tb_logs = {}\n",
    "        tb_logs['valid/loss'] = val_loss\n",
    "        logs['val_loss'] = val_loss\n",
    "        for k, v in self.class_encoded.items():\n",
    "            tb_logs['valid/precision/' + v] = val_precision[k]\n",
    "            tb_logs['valid/recall/' + v] = val_recall[k]\n",
    "            tb_logs['valid/dist_mean/' + v] = val_pos_m if k else val_neg_m\n",
    "            tb_logs['valid/dist_std/' + v] = val_pos_s if k else val_neg_s\n",
    "        \n",
    "        tb_logs['valid/accuracy'] = val_accuracy\n",
    "        logs['val_roc_auc'] = val_roc_auc\n",
    "        tb_logs['valid/roc_auc'] = val_roc_auc\n",
    "\n",
    "        self.tb_writer(tb_logs, wtype='valid', epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = eval(config.optimizer)(learning_rate=config.learning_rate)\n",
    "model.compile(loss=[ContrastiveLoss(model, config.var_loss)], optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet_001'\n",
    "\n",
    "#Save training configuration\n",
    "with open(f'configs/{model_name}.json', 'w') as f:\n",
    "    json.dump(vars(config), f)\n",
    "\n",
    "# Save architecture\n",
    "keras.utils.plot_model(base_network, f'architectures/{model_name}.png', expand_nested=True, show_shapes=True)\n",
    "\n",
    "logdir = os.path.join('logs', model_name)\n",
    "ckpt_dir = os.path.join('checkpoints', model_name)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "ckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(ckpt_dir, 'model.hdf5'),\n",
    "    monitor='val_roc_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "metric_callback = MetricCallback(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def repeat_generator(rlts, batch_size, dataset_period, input_shape, preprocess):\n",
    "    while True:\n",
    "        for e in batched_pairs(rlts, batch_size, dataset_period, input_shape, preprocess, config.train_slice_persons_images):\n",
    "            yield e\n",
    "            \n",
    "model.fit(\n",
    "    repeat_generator(train_rlts, config.batch_size, config.train_dataset_period, config.input_shape, eval(config.preprocess)),\n",
    "    epochs=config.epochs,\n",
    "    steps_per_epoch=train_len//config.batch_size,\n",
    "    callbacks=[metric_callback, ckpt_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission pairs\n",
    "submission_path = 'data/sample_submission.csv'\n",
    "submission_df = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "ckpt_path = 'checkpoints/vggface_002/model.hdf5'\n",
    "model.load_weights(ckpt_path)\n",
    "# model = keras.models.load_model(ckpt_path, custom_objects={'ContrastiveLoss': ContrastiveLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over submission pairs\n",
    "submission_df = submission_df.astype({'is_related': 'float'})\n",
    "is_related = submission_df['is_related']\n",
    "predictions = []\n",
    "for idx, row in submission_df.iterrows():\n",
    "    # Load images\n",
    "    img_pair = row['img_pair']\n",
    "    img1_name, img2_name = img_pair.split('-')\n",
    "    img1_path = os.path.join('data/test', img1_name)\n",
    "    img2_path = os.path.join('data/test', img2_name)\n",
    "    img1 = image.load_img(img1_path, target_size=(config.input_shape[0], config.input_shape[1]))\n",
    "    img2 = image.load_img(img2_path, target_size=(config.input_shape[0], config.input_shape[1]))\n",
    "    img1 = eval(config.preprocess)(np.array(img1).astype('float32'))\n",
    "    img2 = eval(config.preprocess)(np.array(img2).astype('float32'))\n",
    "    img1 = np.expand_dims(img1, 0)\n",
    "    img2 = np.expand_dims(img2, 0)\n",
    "    \n",
    "    # Do an inference, and calculate probability according to distance\n",
    "    y_pred = model.predict({'input_1':img1, 'input_2':img2})\n",
    "    y_pred = y_pred.squeeze()\n",
    "    \n",
    "    lower_bound, upper_bound = get_limits(get_margin(model))\n",
    "    y_prob = dist_to_prob(y_pred, lower_bound, upper_bound)\n",
    "    \n",
    "    predictions.append(y_prob)\n",
    "    is_related[idx] = y_prob\n",
    "    \n",
    "    # Print step\n",
    "    if idx % 100 == 0:\n",
    "        print(f'Processed rows: {idx}')\n",
    "        \n",
    "submission_df.to_csv(f'facenet_015.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(30, 8))\n",
    "plt.hist(predictions, 1000)\n",
    "plt.locator_params(axis='x', nbins=100)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(submission_path)\n",
    "is_related = submission_df['is_related']\n",
    "# print(is_related.sum())\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred < 0.149:\n",
    "        is_related[i] = 1\n",
    "submission_df.to_csv(f'submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
