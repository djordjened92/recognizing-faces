{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 11:28:29.726284: I tensorflow/stream_executor/platform/default/dso_loader.cc:54] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input embeddings\n",
    "\n",
    "The data in the input pickle file is stored in a dictionary structure:\n",
    "```\n",
    "{\n",
    "    [\n",
    "        'FAMILY_ID/PERSON_ID': [EMB_1, EMB_2...EMB_N],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys examples: ['F0475/MID3', 'F0475/MID7', 'F0475/MID6', 'F0475/MID4', 'F0475/MID2']\n",
      "Embeddings shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "with open('../data/train_img_embeddings.pkl', 'rb') as f:\n",
    "       train_embeddings = pickle.load(f)\n",
    "print(f'The keys examples: {list(train_embeddings.keys())[:5]}')\n",
    "\n",
    "embedding_shape = list(list(train_embeddings.values())[0].values())[0].shape\n",
    "print(f'Embeddings shape: {embedding_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pairs generating\n",
    "\n",
    "Available training pairs from csv files are splitted to train - validation sets. Those pairs are positive(there is blood relation). For each set(train/valid) we additionally generate negative pairs.\n",
    "\n",
    "Positive pairs are generated according to the input csv file. For each person of positive pair we create one negative pair.\n",
    "In total we'll have twice more negative than positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embs_pair(pair):\n",
    "    '''\n",
    "    Create pair of embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    p1, p2 -- paths to persons' images directories (familyID/personID)\n",
    "    \n",
    "    Returns:\n",
    "    pairs -- array of image pairs, pairing is alligned to smaller number of images\n",
    "    '''\n",
    "        \n",
    "    p1, p2 = pair\n",
    "    \n",
    "    dir1 = train_embeddings[p1].values()\n",
    "    dir2 = train_embeddings[p2].values()\n",
    "    \n",
    "    for e1 in dir1:\n",
    "        for e2 in dir2:\n",
    "            yield np.concatenate([e1, e2], axis=0)\n",
    "            yield np.concatenate([e2, e1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_set(input_pairs):\n",
    "    for pair, label in input_pairs:\n",
    "        try:\n",
    "            embs = make_embs_pair(pair)\n",
    "            for emb in embs:\n",
    "                yield emb, label\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "def batched_pairs(input_pairs, batch_size, dataset_period):\n",
    "    embs = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    for example in pairs_set(input_pairs):\n",
    "        # Get every nth sample\n",
    "        counter += 1\n",
    "        if counter % dataset_period:\n",
    "            continue\n",
    "        \n",
    "        emb, label = example\n",
    "        embs.append(emb)\n",
    "        labels.append(np.array(label, dtype=int))\n",
    "        if len(labels) == batch_size:\n",
    "            yield np.array(embs), np.array(labels)\n",
    "            embs, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../train_val_set.json', 'r') as f:\n",
    "    train_val_set = json.load(f)\n",
    "\n",
    "train_rlt_list, neg_train_rltshps, valid_rlt_list, neg_valid_rltshps = list(train_val_set.values())\n",
    "train_rlts = list(zip(train_rlt_list + neg_train_rltshps, [True]*len(train_rlt_list) + [False]*len(neg_train_rltshps)))\n",
    "val_rlts = list(zip(valid_rlt_list + neg_valid_rltshps, [True]*len(valid_rlt_list) + [False]*len(neg_valid_rltshps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: (665728,)\n",
      "Valid dataset length: (312672,)\n"
     ]
    }
   ],
   "source": [
    "# Generate train dataset\n",
    "train_x = []\n",
    "train_y = []\n",
    "val_x = []\n",
    "val_y = []\n",
    "for batch in batched_pairs(train_rlts, 32, 1):\n",
    "    train_x.append(batch[0])\n",
    "    train_y.append(batch[1])\n",
    "train_x = np.concatenate(train_x, axis=0)\n",
    "train_y = np.concatenate(train_y)\n",
    "\n",
    "# Permute train data\n",
    "train_idx_perm = np.random.permutation(len(train_x))\n",
    "train_x = train_x[train_idx_perm]\n",
    "train_y = train_y[train_idx_perm]\n",
    "\n",
    "# Generate val dataset\n",
    "for batch in batched_pairs(val_rlts, 32, 1):\n",
    "    val_x.append(batch[0])\n",
    "    val_y.append(batch[1])\n",
    "val_x = np.concatenate(val_x, axis=0)\n",
    "val_y = np.concatenate(val_y)\n",
    "\n",
    "# Permute val data\n",
    "val_idx_perm = np.random.permutation(len(val_x))\n",
    "val_x = val_x[val_idx_perm]\n",
    "val_y = val_y[val_idx_perm]\n",
    "\n",
    "print(f'Train dataset length: {train_y.shape}')\n",
    "print(f'Valid dataset length: {val_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].................................................................................................................................................................................................................................................................................................*..............................*..*\n",
      "optimization finished, #iter = 321094\n",
      "obj = -36837.570737, rho = 0.786942\n",
      "nSV = 263100, nBSV = 214198\n",
      "Total nSV = 263100\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "# SVM\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', class_weight='balanced', probability=True, C=0.2, verbose=True))\n",
    "clf.fit(train_x, train_y)\n",
    "models['SVM'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, solver='saga', C=0.1).fit(train_x, train_y)\n",
    "models['LogisticRegression'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd = QuadraticDiscriminantAnalysis(reg_param=0.5, tol=1e-10).fit(train_x, train_y)\n",
    "models['QuadraticDiscriminantAnalysis'] = qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0).fit(train_x, train_y)\n",
    "models['RandomForestClassifier'] = qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclf = GaussianNB().fit(train_x, train_y)\n",
    "models['GaussianNB'] = qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    train_y_prob = model.predict_proba(train_x)\n",
    "    val_y_prob = model.predict_proba(val_x)\n",
    "    \n",
    "    train_y_predicted = np.argmax(train_y_prob, axis=1)\n",
    "    val_y_predicted = np.argmax(val_y_prob, axis=1)\n",
    "    \n",
    "    # Precision and recall\n",
    "    train_precision, train_recall, _, _ = precision_recall_fscore_support(train_y, train_y_predicted, labels=[0, 1])\n",
    "    train_accuracy = accuracy_score(train_y, train_y_predicted)\n",
    "    valid_precision, valid_recall, _, _ = precision_recall_fscore_support(val_y, val_y_predicted, labels=[0, 1])\n",
    "    valid_accuracy = accuracy_score(val_y, val_y_predicted)\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Train - precision: {train_precision}, recall: {train_recall}, accuracy: {train_accuracy}')\n",
    "    print(f'Valid - precision: {valid_precision}, recall: {valid_recall}, accuracy: {valid_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission pairs\n",
    "submission_path = 'data/sample_submission.csv'\n",
    "submission_df = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "ckpt_path = 'checkpoints/model_6/weights.70-0.11.hdf5'\n",
    "model.load_weights(ckpt_path)\n",
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the threshold according to validation ds\n",
    "val_pred = model.predict([val_pairs[:, 0], val_pairs[:, 1]])\n",
    "val_pos_m, val_pos_s, val_neg_m, val_neg_s = val_distance_stats(val_pred, val_labels.astype(np.int))\n",
    "threshold = ((val_pos_m + val_pos_s) + (val_neg_m - val_neg_s)) / 2\n",
    "\n",
    "# Iterate over submission pairs\n",
    "is_related = submission_df['is_related']\n",
    "predictions = []\n",
    "for idx, row in submission_df.iterrows():\n",
    "    # Load images\n",
    "    img_pair = row['img_pair']\n",
    "    img1_name, img2_name = img_pair.split('-')\n",
    "    img1_path = os.path.join('data/test', img1_name)\n",
    "    img2_path = os.path.join('data/test', img2_name)\n",
    "    img1 = image.load_img(img1_path)\n",
    "    img2 = image.load_img(img2_path)\n",
    "    img1 = np.array(img1).astype('float32')\n",
    "    img2 = np.array(img2).astype('float32')\n",
    "    \n",
    "    # Get FaceNet embeddings\n",
    "    embedding1 = embedder.embeddings([img1])\n",
    "    embedding2 = embedder.embeddings([img2])\n",
    "    \n",
    "    # Do an inference, if distance is smaller than threshold\n",
    "    # then there is the relation\n",
    "    y_pred = model.predict_proba([embedding1, embedding2])\n",
    "    predictions.append(y_pred[0])\n",
    "    is_related[idx] = y_pred[0]\n",
    "    \n",
    "    # Print step\n",
    "    if idx % 100 == 0:\n",
    "        print(f'Processed rows: {idx}')\n",
    "        \n",
    "submission_df.to_csv(f'submission_classic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.85\n",
    "for i, p in enumerate(predictions):\n",
    "    if p < thr:\n",
    "        is_related[i] = 1\n",
    "    else:\n",
    "        is_related[i] = 0\n",
    "submission_df.to_csv(f'submission_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
